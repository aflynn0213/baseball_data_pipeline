# spark-application.yaml
apiVersion: "sparkoperator.k8s.io/v1beta2"
kind: SparkApplication
metadata:
  name: stuff-plus-model
  namespace: default
spec:
  type: Scala  # or Python, depending on your application
  mode: cluster
  image: your_docker_image_name:latest  # Replace with your Docker image name
  mainClass: org.apache.spark.examples.SparkPi  # Example main class
  mainApplicationFile: "local:///opt/spark/examples/jars/spark-examples_2.11-2.4.5.jar"
  sparkVersion: "3.1.1"
  restartPolicy:
    type: Never
  driver:
    cores: 1
    memory: 512m
    labels:
      version: 3.1.1
    serviceAccount: spark
    configMapMounts:
      - name: spark-defaults-config
        mountPath: /opt/bitnami/spark/conf  # Adjust if using a different Spark image
        configMapKeyRef:
          name: spark-defaults-config
          key: spark-defaults.conf
  executor:
    cores: 1
    instances: 2
    memory: 512m
    labels:
      version: 3.1.1
    configMapMounts:
      - name: spark-defaults-config
        mountPath: /opt/bitnami/spark/conf
        configMapKeyRef:
          name: spark-defaults-config
          key: spark-defaults.conf
